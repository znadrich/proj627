---
title: "Final Project"
author: "Zack Nadrich"
date: "12/2/2019"
output: pdf_document
---

```{r setup, include=FALSE}
library(glmnet)
library(tree)
library(MASS)
library(pROC)
knitr::opts_chunk$set(echo = TRUE)
load("../data/renv.RData")
```

## Data Description

The dataset used for this project, Climate Model Simulations Crashes, is from the UCI Machine Learning Repository. The goal for this dataset is determine which combinations of parameter inputs for the climate model simulation causes the simulation to later crash. 

In the frame of scientific computing, we are aiming to reduce wasted computation time for expensive supercomputers as well as reduce debugging time that programmers have to spend in fixing the climate model simulations.

The dataset includes 180 simulations under 3 latin hypercube studies, for a total of 540 observations. The target variable, `outcome`, is coded as `1=success, 0=failure` with an average success rate of `r round(mean(sim$outcome=='1'), 3)`. In addition, there are 18 model parameters that are each scaled to an interval of `[0, 1]`.

Below are the names of each parameter - note that definitions are available in the paper 'Failure Analysis of Parameter-Induced Simulation Crashes in Climate Models'

```{r}
names(sim)
```

## Classification Algorithms

A total of 5 clasification algorithms were evaluated for this dataset

* Logistic regression
* Ridge regression
  * Lambda parameter chosen by CV
* Lasso
  * Lambda parameter chosen by CV
* Linear Discriminant Analysis (LDA)
* Classification tree
  * CV was performed to determine best tree complexity

In preparation to evaluate the algorithms against each other, the data was split into a 80%/20% train/validation split and evaluation was based on the AUC metric.

Below we can see that each algorithm performs similarly, except decision trees. 


```{r, fig.width=5}
auc <- sapply(fits, function(x) round(x$eval, 3))
plot(fits$logistic_reg$roc, col='blue')
plot(fits$lasso$roc, col='red', add=TRUE)
plot(fits$ridge$roc, col='green', add=TRUE)
plot(fits$tree$roc, col='cyan', add=TRUE)
plot(fits$LDA$roc, col='black', add=TRUE)
legend(
  'bottomright', 
  paste(names(fits), auc, sep=' - '), 
  col=c('blue', 'red', 'green', 'cyan', 'black'),
  lty=1
)
```


### Model Inference

#### Logistic Regression

One benefit of logistic regression is the ease in statistical inference, such as the ability to understand the effect each predictor has on the outcome and statistical testing for predictor significance. In our case, we would like to detect which of the input parameters lead to higher chance of a simulation crash, so our needs are met. 

The drawbacks of logistic regression would be that we are assuming a linear relationship of the log-odds and predictors. Should this assumption not hold true, our model will not be flexible enough to detect the true relationship.

As we will see in more clearly in both Lasso and Ridge regression, only a few of the climate model parameter inputs are considered statistically significant in the logistic regression. Based on the performance of Ridge and Lasso, indeed these models will be preferable as they will be more intepretable and as performant as including every climate model parameter.

```{r}
summary(fits$logistic_reg$fit)
```

#### Ridge Regression

In ridge regression we are sacrificing more bias for less variance in model fit. This sacrifice in bias also depends on the relationship being linear, just as with logistic regression. Note that does not shrink coefficient values to 0, so we will not be performing variable selection. We shall see in Lasso that indeed only a few input paramters should be necessary to determine model simulation crashes.

```{r}
fits$ridge$params$coefs
```

#### Lasso

Unlike ridge regression, Lasso does indeed shrink coefficient values to 0. Effectively, this results in a feature selection algorithm that will output which input parameters have the most influence on simulation crashes.

Out of all models evaluated, my preferred is Lasso due to the output below. Simplicity is key here - only two model input parameters determine simulation crashes! Based on definition from the original publications, these are both variable viscosity parameters. 

```{r}
fits$lasso$params$coefs
```

Indeed below we can see that the distribution for the two parameters are different by simulation outcome, where a higher value is more likely to cause a simulation crash.

```{r}
plot(sim$outcome, sim$vconst_2, ylab='outcome', xlab='vconst_2')
plot(sim$outcome, sim$vconst_corr, ylab='outcome', xlab='vconst_corr')
```

#### Linear Discriminate Analysis
 
One benefit of LDA is that the parameterization is intuitive and easy to understand. On the otherhand, due to this parameterization we are making several strong assumptions.

LDA assumptions

* Features are distributed as multivariate normal for each class
* Covariance matrices are homogenous across classes

Given that these assumptions hold true, we will be able to make inference based on the mean vector for each class.

As we have seen before in Lasso, `vconst_corr` and `vconst_2` have a large amount of weight placed on them in the linear discriminant coefficients. Additionally, we can see that these two variables have drastically different means between outcome.

```{r}
fits$LDA$fit
```

#### Classification tree

Classification trees offer a non-parametric model with no assumptions on the data. This flexibility, however, will come at a cost in higher variance. As additional benefits, classification trees will offer variable selection due to the greedy split strategy and a great visualization for model inference.

Of all the algorithms used, classification trees had the worst AUC. This is not to say that the tree is not worthy of being examined as it has the easiest interpretation. Indeed `vconst_corr` and `vconst_2` are picked up as important predictors, along with some other parameters. From the diagram we can see that the tree has segmented simulation crashes as a result of `vconst_corr` > 0.525

```{r}
summary(fits$tree$fit)
```


```{r}
plot(fits$tree$fit)
text(fits$tree$fit, cex=.8)
```

## Conclusion

Failures of simulation models can be costly from both a time and computation perspective. Climate models are some of the most demanding simulations out there and are often run on expensive supercomputers. This case study provides an opportunity to address simulation crashes as a result of invalid or buggy input parameters. 

Due to the nature of this problem, an easy to understand solution is preferrable. As a result, our most preferred method would be Lasso due to the shrinkage penalty. The Lasso model fit reduces our feature set to 2, `vconst_corr` and `vconst_2`. With this information, the computational scientists would be able to troubleshoot the simulation code or place restrictions on input parameters in order to avoid costly simulation crashes.

